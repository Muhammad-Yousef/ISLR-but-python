{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### 6.6 Lab 2: Ridge Regression and the Lasso"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load dataset\n",
    "hitters = pd.read_csv('Data/Hitters.csv', index_col = 0)\n",
    "hitters = hitters.dropna()\n",
    "dummies = pd.get_dummies(hitters[['League', 'Division', 'NewLeague']], drop_first=True)\n",
    "\n",
    "hitters.drop(labels=['League', 'Division', 'NewLeague'], axis=\"columns\", inplace=True)\n",
    "hitters[['League', 'Division', 'NewLeague']] = dummies\n"
   ]
  },
  {
   "source": [
    "### 6.6.1 Ridge Regression\n",
    "We will use sklearne's Ridge method. Here alpha corresponds to lambda in the ISLR notation, it's the hyperparameter for the amount of regularisation.\n",
    "We have chosen to implement the function over a grid of values ranging from alpha = 10^10 to alpha = 10^−2, essentially covering the full range of scenarios from the null model containing\n",
    "only the intercept, to the least squares fit. \n",
    "\n",
    "We expect the coefficient estimates to be much smaller, in terms of L2 norm,\n",
    "when a large value of alpha is used, as compared to when a small value of alpha is\n",
    "used. These are the coefficients when alpha = 11,498, along with their L2 norm:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'The l2 norm is 15.508830341737179'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = hitters.drop(columns=\"Salary\", axis = 1)\n",
    "X = scaler.fit_transform(X)\n",
    "Y = hitters.Salary\n",
    "ridge = Ridge(alpha = 11498)\n",
    "ridge.fit(X, Y)\n",
    "display(f\"The l2 norm is {np.linalg.norm(ridge.coef_)}\")\n"
   ]
  },
  {
   "source": [
    "In contrast, here are the coefficients when Alpha = 705, along with their L2\n",
    "norm. Note the much larger L2 norm of the coefficients associated with this\n",
    "smaller value of alpha."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'The l2 norm is 84.42978999311623'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "X = hitters.drop(columns=\"Salary\", axis = 1)\n",
    "X = scaler.fit_transform(X)\n",
    "Y = hitters.Salary\n",
    "ridge = Ridge(alpha = 705)\n",
    "ridge.fit(X, Y)\n",
    "display(f\"The l2 norm is {np.linalg.norm(ridge.coef_)}\")"
   ]
  },
  {
   "source": [
    "We now split the samples into a training set and a test set in order\n",
    "to estimate the test error of ridge regression and the lasso. Next we fit a ridge regression model on the training set, and evaluate\n",
    "its MSE on the test set, using alpha = 4"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "127015.05249180352"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "X = hitters.drop(columns=\"Salary\", axis = 1)\n",
    "X = scaler.fit_transform(X)\n",
    "Y = hitters.Salary\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=0)\n",
    "\n",
    "ridge = Ridge(alpha = 4)\n",
    "ridge.fit(X_train, y_train)\n",
    "predictions = ridge.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mse"
   ]
  },
  {
   "source": [
    "The test MSE is 127015. Note that if we had instead simply fit a model\n",
    "with just an intercept, we would have predicted each test observation using\n",
    "the mean of the training observations. In that case, we could compute the\n",
    "test set MSE like this:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "231853.4150633351"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "np.mean(np.square(np.mean(y_train)-y_test))"
   ]
  },
  {
   "source": [
    "We could also get the same result by fitting a ridge regression model with\n",
    "a very large value of alpha:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "231853.39962619243"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "ridge = Ridge(alpha = 1e10)\n",
    "ridge.fit(X_train, y_train)\n",
    "predictions = ridge.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mse"
   ]
  },
  {
   "source": [
    "So fitting a ridge regression model with λ = 4 leads to a much lower test\n",
    "MSE than fitting a model with just an intercept. We now check whether\n",
    "there is any benefit to performing ridge regression with λ = 4 instead of\n",
    "just performing least squares regression. Recall that least squares is simply\n",
    "ridge regression with λ = 0"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "134597.49902509098"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "ridge = Ridge(alpha = 0)\n",
    "ridge.fit(X_train, y_train)\n",
    "predictions = ridge.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "mse"
   ]
  },
  {
   "source": [
    "In general, instead of arbitrarily choosing alpha = 4, it would be better to\n",
    "use cross-validation to choose the tuning parameter alpha. We can do this using\n",
    "the built-in cross-validation function. Note that we set a random seed first so our results will\n",
    "be reproducible, since the choice of the cross-validation folds is random."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'Best score is 125945.92026715071 with alpha = 1.023292992280754'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "grid = np.logspace(1e-2, 1e10, 100, endpoint=True)\n",
    "ridgecv = RidgeCV(alphas=grid, cv=10)\n",
    "ridgecv.fit(X_train, y_train)\n",
    "predictions = ridgecv.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "display(f\"Best score is {mse} with alpha = {ridgecv.alpha_}\")\n"
   ]
  },
  {
   "source": [
    "This represents a further improvement over the test MSE that we got using\n",
    "alpha = 4. Finally, we refit our ridge regression model on the full data set,\n",
    "using the value of λ chosen by cross-validation, and examine the coefficient\n",
    "estimates."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-269.91994573,  295.76138136,   17.88982349,  -28.9406864 ,\n",
       "         -8.97697153,  124.2303736 ,  -38.97993038, -222.94459644,\n",
       "        126.98360993,   39.5876684 ,  318.09263198,  159.34003461,\n",
       "       -183.99779792,   78.60149557,   47.33392441,  -23.78702208,\n",
       "         31.01112897,  -60.27326218,  -13.68439816])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "ridge = Ridge(alpha = 1.023292992280754)\n",
    "X = hitters.drop(columns=\"Salary\", axis = 1)\n",
    "X = scaler.fit_transform(X)\n",
    "Y = hitters.Salary\n",
    "ridge.fit(X, Y)\n",
    "ridge.coef_"
   ]
  },
  {
   "source": [
    "As expected, none of the coefficients are zero—ridge regression does not\n",
    "perform variable selection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 6.6.2 The Lasso\n",
    "We saw that ridge regression with a wise choice of λ can outperform least\n",
    "squares as well as the null model on the Hitters data set. We now ask\n",
    "whether the lasso can yield either a more accurate or a more interpretable\n",
    "model than ridge regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "'Best score is 128991.99155773126 with alpha = 1.0023052380778996'"
     },
     "metadata": {}
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "grid = np.logspace(1e-3, 305, 150, endpoint=True)\n",
    "X = hitters.drop(columns=\"Salary\", axis = 1)\n",
    "X = scaler.fit_transform(X)\n",
    "Y = hitters.Salary\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.5, random_state=0)\n",
    "lasso = LassoCV(alphas=grid, cv=10)\n",
    "lasso.fit(X_train, y_train)\n",
    "predictions = lasso.predict(X_test)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "display(f\"Best score is {mse} with alpha = {lasso.alpha_}\")"
   ]
  },
  {
   "source": [
    "This is substantially lower than the test set MSE of the null model and of\n",
    "least squares, and very similar to the test MSE of ridge regression with alpha\n",
    "chosen by cross-validation.\n",
    "However, the lasso has a substantial advantage over ridge regression in\n",
    "that the resulting coefficient estimates are sparse. Here we see that 2 of\n",
    "the 19 coefficient estimates are exactly zero."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([-126.28609652,  103.16157218,   90.71663763,  -14.62400387,\n",
       "         -0.        ,   70.19279164,  -66.18973161, -618.97481896,\n",
       "        705.42415496,   -0.        ,  257.32916914,  -20.39986592,\n",
       "        -13.0852749 ,   41.44428577,   48.92161909,  -60.23940143,\n",
       "         14.13477793,  -43.13284435,   18.18541659])"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "lasso.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}